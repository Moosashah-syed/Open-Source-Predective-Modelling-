Part 1: Jupyter Notebook for Model Training & Evaluation

Copy and paste this into a Jupyter Notebook (or a Python script) to train the model, perform hyperparameter tuning, and save the optimized model with predictions. This notebook includes advanced text preprocessing using Word2Vec and TF-IDF, SMOTE for balancing, a stacking ensemble tuned with Optuna (with a focus on F1-score), and SHAP for interpretability.

# %% [markdown]
# # Complaint Escalation Prediction Model Training
# 
# This notebook builds an optimized model to predict customer complaint escalations. It focuses on achieving high F1-score by using:
# - Advanced text processing (Word2Vec + TF-IDF + sentiment)
# - SMOTE for class imbalance handling
# - A stacking ensemble (LightGBM, XGBoost, CatBoost) with hyperparameter tuning via Optuna
# - SHAP for explainability
# 
# Final deliverables: 
# - Trained model saved as `escalation_model.pkl`
# - Predictions saved as `predictions.csv`
# - Visualizations for model performance

# %% [code]
import pandas as pd
import numpy as np
import re
import string
import joblib
import nltk
import shap
import optuna
import xgboost as xgb
import catboost as cb
import lightgbm as lgb

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from gensim.models import Word2Vec

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
from sklearn.ensemble import StackingClassifier

# Download required NLTK resources
nltk.download("stopwords")
nltk.download("wordnet")

# %% [markdown]
# ## 1. Load and Preprocess Data

# %% [code]
# Load synthetic dataset (make sure the CSV file is in your working directory)
df = pd.read_csv("synthetic_complaint_data.csv")

# For this example, assume df has the following columns:
#   - complaint_description (text)
#   - complaint_type, transaction_frequency (categorical)
#   - Other numeric features like resolution_time_days, sentiment_score, etc.
#   - escalated (target binary variable)

# Encode categorical variables
label_encoders = {}
for col in ["complaint_type", "transaction_frequency"]:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Fill missing values
df.fillna(df.median(), inplace=True)

# %% [markdown]
# ### 1.1 Advanced Text Preprocessing
# - Clean the complaint text (lowercase, remove punctuation/numbers, remove stopwords, lemmatize)
# - Compute sentiment score using TextBlob

# %% [code]
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def clean_text(text):
    try:
        text = text.lower()
        text = re.sub(f"[{string.punctuation}]", "", text)
        text = re.sub("\d+", "", text)
        words = text.split()
        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
        return " ".join(words)
    except Exception as e:
        print("Error in cleaning text:", e)
        return ""

df["cleaned_text"] = df["complaint_description"].astype(str).apply(clean_text)

# Sentiment score as an extra feature
df["sentiment_score"] = df["complaint_description"].astype(str).apply(lambda x: TextBlob(x).sentiment.polarity)

# %% [markdown]
# ### 1.2 Text Feature Extraction using Word2Vec and TF-IDF
# 
# **Word2Vec:** Train a Word2Vec model on the tokenized cleaned text and compute the average vector for each complaint.  
# **TF-IDF:** Create TF-IDF features from the cleaned text.

# %% [code]
# Tokenize text for Word2Vec training
tokenized_texts = [text.split() for text in df["cleaned_text"]]
w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=2, workers=4)

def get_w2v_vector(text):
    words = text.split()
    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(100)

df["w2v_features"] = df["cleaned_text"].apply(get_w2v_vector)
w2v_df = pd.DataFrame(df["w2v_features"].tolist(), columns=[f"w2v_{i}" for i in range(100)])

# TF-IDF features
tfidf = TfidfVectorizer(max_features=500)
tfidf_matrix = tfidf.fit_transform(df["cleaned_text"])
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f"tfidf_{i}" for i in range(500)])

# Merge text features with original data (drop original text columns)
df = pd.concat([df, w2v_df, tfidf_df], axis=1)
df.drop(columns=["complaint_description", "cleaned_text", "w2v_features"], inplace=True)

# %% [markdown]
# ## 2. Handle Imbalance and Feature Scaling
# 
# Use SMOTE to balance classes and StandardScaler for scaling.

# %% [code]
X = df.drop(columns=["escalated"])
y = df["escalated"]

# Balance dataset using SMOTE
smote = SMOTE(sampling_strategy=0.5, random_state=42)
X_smote, y_smote = smote.fit_resample(X, y)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_smote)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_smote, test_size=0.2, random_state=42, stratify=y_smote)

# %% [markdown]
# ## 3. Hyperparameter Tuning with Optuna for LightGBM
# 
# We tune LightGBM parameters as part of our stacking ensemble.

# %% [code]
def objective(trial):
    params = {
        "objective": "binary",
        "boosting_type": "gbdt",
        "num_leaves": trial.suggest_int("num_leaves", 20, 300),
        "max_depth": trial.suggest_int("max_depth", 3, 15),
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.3),
        "n_estimators": trial.suggest_int("n_estimators", 50, 500),
        "reg_lambda": trial.suggest_loguniform("reg_lambda", 1e-3, 10),
        "reg_alpha": trial.suggest_loguniform("reg_alpha", 1e-3, 10),
    }
    model = lgb.LGBMClassifier(**params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return f1_score(y_test, y_pred)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)
best_params = study.best_params
print("Best Params:", best_params)

# %% [markdown]
# ## 4. Build a Stacking Ensemble Model
# 
# We'll stack three models: LightGBM (tuned), XGBoost, and CatBoost. This ensemble should provide robust performance focusing on F1-score.

# %% [code]
lgb_model = lgb.LGBMClassifier(**best_params)
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric="logloss")
cat_model = cb.CatBoostClassifier(verbose=0)

stack_model = StackingClassifier(
    estimators=[("lgbm", lgb_model), ("xgb", xgb_model), ("cat", cat_model)],
    final_estimator=lgb.LGBMClassifier(n_estimators=100),
    cv=5
)

stack_model.fit(X_train, y_train)

# %% [markdown]
# ## 5. Evaluate the Model

# %% [code]
y_pred = stack_model.predict(X_test)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("Final F1 Score:", f1)
print("AUC-ROC:", auc)
print("Confusion Matrix:\n", cm)

# SHAP Explanation
explainer = shap.Explainer(stack_model, X_train)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)

# %% [markdown]
# ## 6. Save the Model and Predictions

# %% [code]
joblib.dump(stack_model, "escalation_model.pkl")
print("Model saved as escalation_model.pkl")

# Save predictions (you can include additional info if desired)
predictions_df = pd.DataFrame({"Complaint_ID": np.arange(len(y_pred)), "Predicted_Escalated": y_pred})
predictions_df.to_csv("predictions.csv", index=False)
print("Predictions saved as predictions.csv")


---

Part 2: Flask API for Real-Time Prediction

Save the code below as app.py. This Flask API loads the saved model and scaler, then provides an endpoint to receive complaint details (in JSON) and return the escalation prediction.

from flask import Flask, request, jsonify
import joblib
import numpy as np
import pandas as pd
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler

# Ensure NLTK resources are downloaded
nltk.download("stopwords")
nltk.download("wordnet")

app = Flask(__name__)

# Load model and scaler
model = joblib.load("escalation_model.pkl")
scaler = joblib.load("scaler.pkl") if False else None  # If you saved scaler separately

# For simplicity, reload your Word2Vec and TF-IDF models if needed.
# In this example, we'll reinitialize them as in training. In production, you should save these objects.
# Reinitialize text processing tools:
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = text.lower()
    text = re.sub(f"[{string.punctuation}]", "", text)
    text = re.sub("\d+", "", text)
    words = text.split()
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return " ".join(words)

# For Word2Vec: Load your trained Word2Vec model
# For demonstration, assume you have saved it as 'w2v.model'
w2v_model = joblib.load("w2v_model.pkl") if False else None

def get_w2v_vector(text, w2v_model):
    words = text.split()
    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(100)

# For TF-IDF: Load your fitted TF-IDF vectorizer (if saved)
tfidf = joblib.load("tfidf_vectorizer.pkl") if False else None

# For this API, we assume the request JSON includes:
#  - complaint_description: text description of the complaint
#  - Other features (if needed): complaint_type, transaction_frequency, etc.
@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json(force=True)
        # Extract text and process it
        complaint_text = data.get("complaint_description", "")
        cleaned = clean_text(complaint_text)
        
        # Compute sentiment score
        sentiment = TextBlob(complaint_text).sentiment.polarity
        
        # Get Word2Vec vector (if w2v_model is available)
        if w2v_model:
            w2v_vec = get_w2v_vector(cleaned, w2v_model)
        else:
            w2v_vec = np.zeros(100)
        
        # TF-IDF features
        if tfidf:
            tfidf_vec = tfidf.transform([cleaned]).toarray().flatten()
        else:
            # If TF-IDF vectorizer is not loaded, use zeros
            tfidf_vec = np.zeros(500)
        
        # Assume other features are provided in the JSON:
        # e.g., complaint_type, transaction_frequency, etc.
        # For demonstration, we extract them directly.
        complaint_type = data.get("complaint_type", 0)
        transaction_frequency = data.get("transaction_frequency", 0)
        # If there are other numeric features, extract them similarly.
        
        # Combine all features into one vector.
        # You must follow the same order and scaling as in training.
        numeric_features = [sentiment, complaint_type, transaction_frequency]  # add other features if available
        # Concatenate numeric features with Word2Vec and TF-IDF features.
        feature_vector = np.concatenate([np.array(numeric_features), w2v_vec, tfidf_vec])
        feature_vector = feature_vector.reshape(1, -1)
        
        # Scale features if scaler is available
        if scaler:
            feature_vector = scaler.transform(feature_vector)
        
        # Predict
        prediction = model.predict(feature_vector)[0]
        result = {"escalated": int(prediction)}
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)})

if __name__ == '__main__':
    app.run(debug=True)

> Note: In production, you should save and load your preprocessing objects (scaler, Word2Vec, TF-IDF vectorizer) rather than reinitializing defaults.




---

Part 3: Streamlit Dashboard for Demo

Save the code below as streamlit_app.py. This dashboard allows users to input complaint details and see the predicted escalation, along with visualizations of model performance.

import streamlit as st
import requests
import json
import joblib
import pandas as pd
import matplotlib.pyplot as plt
import shap
import numpy as np

st.title("Complaint Escalation Prediction Dashboard")

st.markdown("""
This dashboard allows you to enter complaint details and get real-time predictions on whether the complaint will be escalated.
""")

# Input fields for complaint details
complaint_text = st.text_area("Enter Complaint Description", "Type your complaint here...")
complaint_type = st.number_input("Complaint Type (Encoded value)", min_value=0, value=0)
transaction_frequency = st.number_input("Transaction Frequency (Encoded value)", min_value=0, value=0)

# When the Predict button is clicked, send data to the Flask API
if st.button("Predict Escalation"):
    payload = {
        "complaint_description": complaint_text,
        "complaint_type": complaint_type,
        "transaction_frequency": transaction_frequency
    }
    try:
        response = requests.post("http://localhost:5000/predict", json=payload)
        result = response.json()
        if "escalated" in result:
            st.success(f"Predicted Escalation: {result['escalated']}")
        else:
            st.error("Prediction error: " + result.get("error", "Unknown error"))
    except Exception as e:
        st.error("Error connecting to API: " + str(e))

# Additional section: Model Performance Visualizations
st.markdown("## Model Performance Visualizations")

# Load saved predictions and show confusion matrix if available
try:
    preds = pd.read_csv("predictions.csv")
    st.write("Sample Predictions:")
    st.dataframe(preds.head())
except Exception as e:
    st.write("Predictions file not found.")

# SHAP summary plot (for illustration, load SHAP values if available)
# Here we load the model and data to generate a SHAP plot.
try:
    model = joblib.load("escalation_model.pkl")
    # For demo, assume you have a sample X_test (load from file or use dummy data)
    sample_data = np.load("sample_X_test.npy") if False else None
    if sample_data is not None:
        explainer = shap.Explainer(model, sample_data)
        shap_values = explainer(sample_data)
        st.pyplot(shap.summary_plot(shap_values, sample_data, show=False))
except Exception as e:
    st.write("SHAP plot could not be generated:", e)

> Note:

Ensure that the Flask API is running (default on http://localhost:5000) before using the Streamlit app.

For the SHAP plot, if you saved sample data (e.g., X_test) during model training, load it here; otherwise, you can remove that section or use a placeholder.





---

Final Steps

1. Run the Jupyter Notebook to train and save your model:

This produces escalation_model.pkl and predictions.csv (and optionally saves preprocessing objects if you extend the code).



2. Run the Flask API:

Open a terminal and execute:

python app.py

The API should run on http://localhost:5000.



3. Run the Streamlit Dashboard:

Open a terminal and execute:

streamlit run streamlit_app.py

Use the dashboard to input complaint details and see the predictions and visualization
